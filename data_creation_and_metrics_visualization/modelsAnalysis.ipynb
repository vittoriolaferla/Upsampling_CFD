{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import re\n",
    "import logging\n",
    "\n",
    "\n",
    "def compute_average_deviation(deviation_frequency, deviation_sum, velocity_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Compute the average magnitude of deviations per velocity bin for each dataset,\n",
    "    considering only velocities above a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): {dataset_name: {bin_label: count}}\n",
    "        deviation_sum (dict): {dataset_name: {bin_label: sum_of_deviations}}\n",
    "        velocity_threshold (float): The minimum ground truth velocity to consider (in m/s).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Velocity Bin' as index and datasets as columns containing average deviations.\n",
    "    \"\"\"\n",
    "    # Extract all unique bin labels across all datasets\n",
    "    bin_labels = set()\n",
    "    for dataset in deviation_frequency:\n",
    "        bin_labels.update(deviation_frequency[dataset].keys())\n",
    "    # Convert all bin_labels to strings if they aren't already\n",
    "    bin_labels = sorted(bin_labels, key=lambda x: float(x.split('-')[0]) if isinstance(x, str) else float(x))\n",
    "\n",
    "    # Initialize DataFrame\n",
    "    avg_dev_df = pd.DataFrame(index=bin_labels)\n",
    "\n",
    "    for dataset in deviation_frequency:\n",
    "        avg_devs = []\n",
    "        for bin_label in bin_labels:\n",
    "            # Extract the lower bound of the bin\n",
    "            if isinstance(bin_label, str):\n",
    "                try:\n",
    "                    lower_str = bin_label.split('-')[0]\n",
    "                    lower = float(lower_str)\n",
    "                except (IndexError, ValueError) as e:\n",
    "                    print(f\"Invalid bin label format: '{bin_label}'. Error: {e}. Assigning average deviation as 0.0.\")\n",
    "                    avg_devs.append(0.0)\n",
    "                    continue\n",
    "            elif isinstance(bin_label, (float, int)):\n",
    "                lower = float(bin_label)\n",
    "            else:\n",
    "                print(f\"Unsupported bin label type: {type(bin_label)}. Assigning average deviation as 0.0.\")\n",
    "                avg_devs.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Apply velocity threshold\n",
    "            if lower < velocity_threshold:\n",
    "                avg_devs.append(None)  # Use None to indicate exclusion\n",
    "                continue\n",
    "\n",
    "            freq = deviation_frequency[dataset].get(bin_label, 0)\n",
    "            sum_dev = deviation_sum[dataset].get(bin_label, 0.0)\n",
    "            avg = sum_dev / freq if freq > 0 else 0.0\n",
    "            avg_devs.append(avg)\n",
    "        avg_dev_df[dataset] = avg_devs\n",
    "\n",
    "    # Reset index to have 'Velocity Bin' as a column\n",
    "    avg_dev_df = avg_dev_df.reset_index().rename(columns={'index': 'Velocity Bin'})\n",
    "\n",
    "    # Drop bins below the threshold (i.e., where any dataset has None)\n",
    "    if velocity_threshold > 0.0:\n",
    "        # Only keep rows where all datasets have non-None values\n",
    "        avg_dev_df = avg_dev_df.dropna(subset=avg_dev_df.columns[1:])\n",
    "        # Alternatively, if you want to keep bins where at least one dataset meets the threshold:\n",
    "        # avg_dev_df = avg_dev_df.dropna(subset=['Velocity Bin'])\n",
    "        # However, since we assign None to bins below the threshold for all datasets,\n",
    "        # dropping rows with any NaN should suffice.\n",
    "    \n",
    "    return avg_dev_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_error_metrics(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Compute common error metrics between predicted and ground truth velocity fields.\n",
    "\n",
    "    Args:\n",
    "        predicted (numpy array): Predicted velocity values.\n",
    "        ground_truth (numpy array): Ground truth velocity values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing MAE, MSE, RMSE, and R-squared.\n",
    "    \"\"\"\n",
    "    predicted_flat = predicted.flatten()\n",
    "    ground_truth_flat = ground_truth.flatten()\n",
    "\n",
    "    mae = mean_absolute_error(ground_truth_flat, predicted_flat)\n",
    "    mse = mean_squared_error(ground_truth_flat, predicted_flat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(ground_truth_flat, predicted_flat)\n",
    "\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "\n",
    "def comprehensive_analysis(predicted_image_path, ground_truth_image_path, velocity_threshold=0.5, deviation_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Perform comprehensive analysis between predicted and ground truth velocity fields.\n",
    "\n",
    "    Args:\n",
    "        predicted_image_path (str): Path to the predicted velocity field image.\n",
    "        ground_truth_image_path (str): Path to the ground truth velocity field image.\n",
    "        velocity_threshold (float): Threshold for high-velocity regions (m/s).\n",
    "        deviation_threshold (float): Threshold to define a significant deviation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (error_metrics, aggregate_data)\n",
    "            - error_metrics (dict): Computed error metrics.\n",
    "            - aggregate_data (dict): Data accumulated for aggregate plots.\n",
    "    \"\"\"\n",
    "    # Extract velocity fields\n",
    "    try:\n",
    "        predicted_field = np.array(Image.open(predicted_image_path).convert('L')) / 255.0\n",
    "        ground_truth_field = np.array(Image.open(ground_truth_image_path).convert('L')) / 255.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading images:\\nPredicted: {predicted_image_path}\\nGround Truth: {ground_truth_image_path}\\nException: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Compute centerline profiles\n",
    "    def extract_centerline(velocity_field):\n",
    "        center_column = velocity_field.shape[1] // 2\n",
    "        centerline = velocity_field[:, center_column]\n",
    "        y = np.arange(velocity_field.shape[0])\n",
    "        return y, centerline\n",
    "\n",
    "    y_pred, pred_centerline = extract_centerline(predicted_field)\n",
    "    y_gt, gt_centerline = extract_centerline(ground_truth_field)\n",
    "\n",
    "    # Compute deviations\n",
    "    deviations = pred_centerline - gt_centerline\n",
    "\n",
    "    # Compute error metrics\n",
    "    error_metrics = compute_error_metrics(pred_centerline, gt_centerline)\n",
    "\n",
    "    # Define where deviations occur based on the threshold\n",
    "    deviation_mask = np.abs(deviations) > deviation_threshold\n",
    "\n",
    "    # Accumulate data for aggregate plots\n",
    "    aggregate_data = {\n",
    "        \"overall_deviations\": deviations,\n",
    "        \"high_velocity_deviations\": deviations[gt_centerline > velocity_threshold],\n",
    "        \"deviation_histogram\": deviations,\n",
    "        \"deviation_mask\": deviation_mask,\n",
    "        \"ground_truth_velocities\": gt_centerline\n",
    "    }\n",
    "\n",
    "    return error_metrics, aggregate_data\n",
    "\n",
    "def process_image(dataset_name, image_name, model_folder, ground_truth_folder, velocity_threshold, deviation_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Process a single image: perform analysis and accumulate data.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        image_name (str): Filename of the image.\n",
    "        model_folder (str): Path to the model's prediction folder.\n",
    "        ground_truth_folder (str): Path to the ground truth images folder.\n",
    "        velocity_threshold (float): Threshold for high-velocity regions (m/s).\n",
    "        deviation_threshold (float): Threshold to define a significant deviation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (metrics_record, aggregate_data) or None if skipped.\n",
    "            - metrics_record (dict): Error metrics for the image.\n",
    "            - aggregate_data (dict): Data for aggregate plots.\n",
    "    \"\"\"\n",
    "    predicted_image_path = os.path.join(model_folder, image_name)\n",
    "    ground_truth_image_path = os.path.join(ground_truth_folder, image_name)\n",
    "\n",
    "    if not os.path.exists(ground_truth_image_path):\n",
    "        print(f\"Ground truth for '{image_name}' not found in dataset '{dataset_name}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Perform comprehensive analysis\n",
    "    error_metrics, aggregate_data = comprehensive_analysis(\n",
    "        predicted_image_path,\n",
    "        ground_truth_image_path,\n",
    "        velocity_threshold=velocity_threshold,\n",
    "        deviation_threshold=deviation_threshold\n",
    "    )\n",
    "\n",
    "    if error_metrics is None or aggregate_data is None:\n",
    "        print(f\"Analysis failed for '{image_name}' in dataset '{dataset_name}'. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Prepare metrics record\n",
    "    metrics_record = {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Image\": image_name,\n",
    "        \"MAE\": error_metrics[\"MAE\"],\n",
    "        \"MSE\": error_metrics[\"MSE\"],\n",
    "        \"RMSE\": error_metrics[\"RMSE\"],\n",
    "        \"R2\": error_metrics[\"R2\"]\n",
    "    }\n",
    "\n",
    "    return (metrics_record, aggregate_data)\n",
    "\n",
    "def batch_comprehensive_analysis_parallel(dataset_paths, ground_truth_folder, velocity_threshold=0.5, deviation_threshold=0.01, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Perform comprehensive analysis for multiple datasets against the same ground truth using parallel processing.\n",
    "\n",
    "    Args:\n",
    "        dataset_paths (dict): Dictionary with dataset names as keys and model prediction folder paths as values.\n",
    "        ground_truth_folder (str): Path to the folder containing ground truth images.\n",
    "        velocity_threshold (float): Threshold for high-velocity regions (m/s).\n",
    "        deviation_threshold (float): Threshold to define a significant deviation.\n",
    "        n_jobs (int): Number of jobs for parallel processing. -1 means using all processors.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains average_metrics_df and accumulated data for aggregate plots.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store metrics\n",
    "    metrics_list = []\n",
    "\n",
    "    # Initialize dictionaries to accumulate data for aggregate plots\n",
    "    aggregate_deviations = {}\n",
    "    aggregate_high_velocity_deviations = {}\n",
    "    aggregate_deviation_histograms = {}\n",
    "    \n",
    "    # Initialize dictionaries for deviation frequency and sum per velocity bin\n",
    "    deviation_frequency = {}\n",
    "    deviation_sum = {}\n",
    "    \n",
    "    # Initialize a dictionary for ground truth velocity density\n",
    "    ground_truth_density = {}\n",
    "    for dataset in dataset_paths:\n",
    "        ground_truth_density[dataset] = []\n",
    "\n",
    "    # Prepare tasks\n",
    "    tasks = []\n",
    "    for dataset_name, model_folder in dataset_paths.items():\n",
    "        print(f\"\\nPreparing to analyze Dataset: '{dataset_name}'\")\n",
    "        if not os.path.isdir(model_folder):\n",
    "            print(f\"Model prediction folder '{model_folder}' for dataset '{dataset_name}' does not exist. Skipping.\")\n",
    "            continue\n",
    "        for image_name in os.listdir(model_folder):\n",
    "            if image_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tif', '.tiff')):\n",
    "                tasks.append((dataset_name, image_name, model_folder, ground_truth_folder, velocity_threshold, deviation_threshold))\n",
    "\n",
    "    # Execute tasks in parallel\n",
    "    print(f\"\\nStarting parallel processing with {n_jobs if n_jobs != -1 else 'all available'} jobs...\")\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_image)(*task) for task in tasks\n",
    "    )\n",
    "\n",
    "    # Process results\n",
    "    for result in results:\n",
    "        if result is None:\n",
    "            continue\n",
    "        metrics_record, aggregate_data = result\n",
    "        metrics_list.append(metrics_record)\n",
    "\n",
    "        dataset_name = metrics_record[\"Dataset\"]\n",
    "\n",
    "        # Accumulate deviations\n",
    "        if dataset_name not in aggregate_deviations:\n",
    "            aggregate_deviations[dataset_name] = aggregate_data[\"overall_deviations\"]\n",
    "        else:\n",
    "            aggregate_deviations[dataset_name] = np.concatenate((aggregate_deviations[dataset_name],\n",
    "                                                                 aggregate_data[\"overall_deviations\"]))\n",
    "\n",
    "        # Accumulate high-velocity deviations\n",
    "        if dataset_name not in aggregate_high_velocity_deviations:\n",
    "            aggregate_high_velocity_deviations[dataset_name] = aggregate_data[\"high_velocity_deviations\"]\n",
    "        else:\n",
    "            aggregate_high_velocity_deviations[dataset_name] = np.concatenate((aggregate_high_velocity_deviations[dataset_name],\n",
    "                                                                                 aggregate_data[\"high_velocity_deviations\"]))\n",
    "\n",
    "        # Accumulate deviation histograms\n",
    "        if dataset_name not in aggregate_deviation_histograms:\n",
    "            aggregate_deviation_histograms[dataset_name] = aggregate_data[\"deviation_histogram\"]\n",
    "        else:\n",
    "            aggregate_deviation_histograms[dataset_name] = np.concatenate((aggregate_deviation_histograms[dataset_name],\n",
    "                                                                           aggregate_data[\"deviation_histogram\"]))\n",
    "\n",
    "        # Accumulate deviation frequencies and sums\n",
    "        if dataset_name not in deviation_frequency:\n",
    "            deviation_frequency[dataset_name] = {}\n",
    "        if dataset_name not in deviation_sum:\n",
    "            deviation_sum[dataset_name] = {}\n",
    "        \n",
    "        gt_velocities = aggregate_data[\"ground_truth_velocities\"]\n",
    "        deviation_mask = aggregate_data[\"deviation_mask\"]\n",
    "        deviations = aggregate_data[\"overall_deviations\"]\n",
    "\n",
    "        for gt_v, dev, dev_abs in zip(gt_velocities, deviation_mask, np.abs(deviations)):\n",
    "            # Collect ground truth velocities for density plot\n",
    "            ground_truth_density[dataset_name].append(gt_v)\n",
    "\n",
    "            if dev:\n",
    "                # Round ground truth velocity to 2 decimal places to match binning\n",
    "                gt_v_rounded = round(gt_v, 2)\n",
    "                # Update frequency\n",
    "                if gt_v_rounded in deviation_frequency[dataset_name]:\n",
    "                    deviation_frequency[dataset_name][gt_v_rounded] += 1\n",
    "                else:\n",
    "                    deviation_frequency[dataset_name][gt_v_rounded] = 1\n",
    "                # Update sum of deviations\n",
    "                if gt_v_rounded in deviation_sum[dataset_name]:\n",
    "                    deviation_sum[dataset_name][gt_v_rounded] += dev_abs\n",
    "                else:\n",
    "                    deviation_sum[dataset_name][gt_v_rounded] = dev_abs\n",
    "\n",
    "    # Create DataFrame from the list\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "    # Debugging: Inspect the first few entries of metrics_df\n",
    "    print(\"\\n--- Metrics DataFrame Head ---\")\n",
    "    print(metrics_df.head())\n",
    "\n",
    "    print(\"\\n--- Metrics DataFrame Info ---\")\n",
    "    print(metrics_df.info())\n",
    "\n",
    "    # Data Validation: Ensure all required columns are present and have correct data types\n",
    "    required_columns = [\"Dataset\", \"Image\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"]\n",
    "    missing_columns = set(required_columns) - set(metrics_df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following required columns are missing from the metrics DataFrame: {missing_columns}\")\n",
    "\n",
    "    # Check for non-numeric data in error metrics columns\n",
    "    for col in [\"MAE\", \"MSE\", \"RMSE\", \"R2\"]:\n",
    "        if not pd.api.types.is_numeric_dtype(metrics_df[col]):\n",
    "            raise TypeError(f\"Column '{col}' must contain numeric data. Found data type: {metrics_df[col].dtype}\")\n",
    "\n",
    "    # Inspect the 'Dataset' column for any anomalies\n",
    "    print(\"\\n--- Unique Dataset Names ---\")\n",
    "    print(metrics_df['Dataset'].unique())\n",
    "\n",
    "    print(\"\\n--- Entries in 'Dataset' Containing Image Filenames ---\")\n",
    "    image_extensions = ['.png', '.jpg', '.jpeg', '.tif', '.tiff']\n",
    "    pattern = '|'.join([ext.replace('.', r'\\.') for ext in image_extensions])\n",
    "    problematic_entries = metrics_df[metrics_df['Dataset'].str.contains(pattern, regex=True)]\n",
    "    print(problematic_entries)\n",
    "\n",
    "    print(\"\\n--- Length of 'Dataset' Entries ---\")\n",
    "    print(metrics_df['Dataset'].str.len().describe())\n",
    "\n",
    "    # Check for any unusually long 'Dataset' entries\n",
    "    unusually_long = metrics_df[metrics_df['Dataset'].str.len() > 50]\n",
    "    if not unusually_long.empty:\n",
    "        print(\"\\n--- Unusually Long 'Dataset' Entries ---\")\n",
    "        print(unusually_long)\n",
    "    else:\n",
    "        print(\"\\nNo unusually long 'Dataset' entries found.\")\n",
    "\n",
    "    # Remove entries where 'Dataset' contains image filenames\n",
    "    if not problematic_entries.empty:\n",
    "        print(\"\\nRemoving problematic entries from 'metrics_df'...\")\n",
    "        metrics_df = metrics_df[~metrics_df['Dataset'].str.contains(pattern, regex=True)]\n",
    "        print(f\"Remaining entries after removal: {metrics_df.shape[0]}\")\n",
    "    else:\n",
    "        print(\"\\nNo problematic 'Dataset' entries found. Proceeding...\")\n",
    "\n",
    "    # Compute average error metrics per dataset by selecting only numeric columns\n",
    "    try:\n",
    "        average_metrics = metrics_df.groupby(\"Dataset\")[[\"MAE\", \"MSE\", \"RMSE\", \"R2\"]].mean().reset_index()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during groupby operation: {e}\")\n",
    "        raise\n",
    "\n",
    "    return {\n",
    "        \"average_metrics_df\": average_metrics,\n",
    "        \"aggregate_deviations\": aggregate_deviations,\n",
    "        \"aggregate_high_velocity_deviations\": aggregate_high_velocity_deviations,\n",
    "        \"aggregate_deviation_histograms\": aggregate_deviation_histograms,\n",
    "        \"deviation_frequency\": deviation_frequency,\n",
    "        \"deviation_sum\": deviation_sum,\n",
    "        \"ground_truth_density\": ground_truth_density\n",
    "    }\n",
    "\n",
    "def plot_fixed_velocity_bin_metrics(deviation_frequency, deviation_sum, output_path, bin_label=\"0.00-0.10 m/s\"):\n",
    "    \"\"\"\n",
    "    Plot frequency, sum, and density of deviations within a specific velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                                    mapping ground truth velocity to deviation count.\n",
    "        deviation_sum (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                              mapping ground truth velocity to sum of deviation magnitudes.\n",
    "        output_path (str): Path to save the fixed velocity bin metrics plot.\n",
    "        bin_label (str): The specific velocity bin to focus on (e.g., \"0.00-0.10 m/s\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define the specific velocity bin\n",
    "    target_bin = bin_label  # e.g., \"0.00-0.10 m/s\"\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for dataset in deviation_frequency:\n",
    "        # Extract the lower and upper bounds from bin_label\n",
    "        lower, upper = map(float, target_bin.split('-'))\n",
    "        # Since bins are defined with two decimal places, ensure consistency\n",
    "        lower = round(lower, 2)\n",
    "        upper = round(upper, 2)\n",
    "\n",
    "        # Count frequency\n",
    "        freq = 0\n",
    "        for v, count in deviation_frequency[dataset].items():\n",
    "            if lower <= v < upper:\n",
    "                freq += count\n",
    "\n",
    "        # Sum of deviations\n",
    "        sum_dev = 0.0\n",
    "        for v, s_dev in deviation_sum[dataset].items():\n",
    "            if lower <= v < upper:\n",
    "                sum_dev += s_dev\n",
    "\n",
    "        # Total deviations for density\n",
    "        total_devs = sum(deviation_frequency[dataset].values())\n",
    "        density = (freq / total_devs) * 100 if total_devs > 0 else 0\n",
    "\n",
    "        plot_data.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Frequency\": freq,\n",
    "            \"Sum of Deviations\": sum_dev,\n",
    "            \"Density (%)\": density\n",
    "        })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Set the order for metrics\n",
    "    metrics_order = [\"Frequency\", \"Sum of Deviations\", \"Density (%)\"]\n",
    "\n",
    "    # Melt the DataFrame for seaborn plotting\n",
    "    plot_df_melted = plot_df.melt(id_vars=\"Dataset\", value_vars=metrics_order, var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=plot_df_melted, x=\"Dataset\", y=\"Value\", hue=\"Metric\")\n",
    "    plt.xlabel(\"Dataset\", fontsize=14)\n",
    "    plt.ylabel(\"Value\", fontsize=14)\n",
    "    plt.title(f\"Deviation Metrics within {target_bin} Velocity Bin\", fontsize=16)\n",
    "    plt.legend(title=\"Metric\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_average_deviation_per_bin(avg_dev_df, output_path, velocity_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Plot the average magnitude of deviations per velocity bin for each dataset,\n",
    "    considering only velocities above a specified threshold.\n",
    "\n",
    "    Args:\n",
    "        avg_dev_df (pd.DataFrame): DataFrame with 'Velocity Bin' and datasets as columns containing average deviations.\n",
    "        output_path (str): Path to save the average deviation plot.\n",
    "        velocity_threshold (float): The minimum ground truth velocity to consider (in m/s).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Melt the DataFrame for seaborn\n",
    "    plot_df_melted = avg_dev_df.melt(id_vars=\"Velocity Bin\", var_name=\"Dataset\", value_name=\"Average Deviation (m/s)\")\n",
    "    \n",
    "    # Convert 'Velocity Bin' to numeric by extracting the lower bound\n",
    "    plot_df_melted['Velocity Lower Bound'] = plot_df_melted['Velocity Bin'].apply(\n",
    "        lambda x: float(x.split('-')[0]) if isinstance(x, str) else float(x)\n",
    "    )\n",
    "    \n",
    "    # Sort by 'Velocity Lower Bound' for accurate plotting\n",
    "    plot_df_melted = plot_df_melted.sort_values('Velocity Lower Bound')\n",
    "    \n",
    "    # Initialize the seaborn line plot\n",
    "    sns.lineplot(\n",
    "        data=plot_df_melted,\n",
    "        x='Velocity Lower Bound',\n",
    "        y='Average Deviation (m/s)',\n",
    "        hue='Dataset',\n",
    "        marker='o'\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(\"Ground Truth Velocity Lower Bound (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Average Deviation Magnitude (m/s)\", fontsize=14)\n",
    "    plt.title(f\"Average Magnitude of Deviations per Dataset (Above {velocity_threshold} m/s)\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Set x-axis limits starting from the velocity threshold\n",
    "    plt.xlim(velocity_threshold, 0.56)  # Adjust 0.56 if your max velocity changes\n",
    "    \n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Average deviation per bin plot saved to '{output_path}'.\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_deviation_density_per_velocity_bin(deviation_frequency, output_path, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Plot the density of deviations per ground truth velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                                    mapping ground truth velocity to deviation count.\n",
    "        output_path (str): Path to save the deviation density plot.\n",
    "        bin_size (float): Size of each velocity bin.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define fixed velocity bins from 0 to 0.56 m/s with bin size 0.05 m/s\n",
    "    min_v = 0.0\n",
    "    max_v = 0.56\n",
    "    bins = np.arange(min_v, max_v + bin_size, bin_size)\n",
    "    bin_labels = [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
    "\n",
    "    # Prepare DataFrame for plotting\n",
    "    plot_data = []\n",
    "    for dataset, freq_dict in deviation_frequency.items():\n",
    "        # Initialize counts for each bin\n",
    "        bin_counts = np.zeros(len(bins)-1, dtype=int)\n",
    "        for v, count in freq_dict.items():\n",
    "            # Find the appropriate bin\n",
    "            bin_index = np.digitize(v, bins) - 1  # digitize returns indices starting at 1\n",
    "            if 0 <= bin_index < len(bin_counts):\n",
    "                bin_counts[bin_index] += count\n",
    "        # Calculate total deviations for density normalization\n",
    "        total_deviations = bin_counts.sum()\n",
    "        if total_deviations > 0:\n",
    "            bin_density = (bin_counts / total_deviations) * 100  # Percentage\n",
    "        else:\n",
    "            bin_density = bin_counts  # All zeros\n",
    "        for i in range(len(bin_density)):\n",
    "            plot_data.append({\n",
    "                \"Dataset\": dataset,\n",
    "                \"Velocity Bin\": bin_labels[i],\n",
    "                \"Density (%)\": bin_density[i]\n",
    "            })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Pivot the DataFrame for easier plotting\n",
    "    pivot_df = plot_df.pivot(index=\"Velocity Bin\", columns=\"Dataset\", values=\"Density (%)\").fillna(0)\n",
    "\n",
    "    # Plotting\n",
    "    pivot_df.plot(kind='bar', figsize=(15, 8))\n",
    "    plt.xlabel(\"Ground Truth Velocity Bin (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Density of Deviations (%)\", fontsize=14)\n",
    "    plt.title(\"Density of Velocity Deviations per Ground Truth Velocity Bin with Treshold=0.01\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_sum_deviation_per_velocity_bin(deviation_sum, output_path, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Plot the sum of deviation magnitudes per ground truth velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_sum (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                              mapping ground truth velocity to sum of deviation magnitudes.\n",
    "        output_path (str): Path to save the sum deviation plot.\n",
    "        bin_size (float): Size of each velocity bin.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define fixed velocity bins from 0 to 0.56 m/s with bin size 0.05 m/s\n",
    "    min_v = 0.0\n",
    "    max_v = 0.56\n",
    "    bins = np.arange(min_v, max_v + bin_size, bin_size)\n",
    "    bin_labels = [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
    "\n",
    "    # Prepare DataFrame for plotting\n",
    "    plot_data = []\n",
    "    for dataset, sum_dict in deviation_sum.items():\n",
    "        # Initialize sums for each bin\n",
    "        bin_sums = np.zeros(len(bins)-1, dtype=float)\n",
    "        for v, sum_dev in sum_dict.items():\n",
    "            # Find the appropriate bin\n",
    "            bin_index = np.digitize(v, bins) - 1  # digitize returns indices starting at 1\n",
    "            if 0 <= bin_index < len(bin_sums):\n",
    "                bin_sums[bin_index] += sum_dev\n",
    "        for i in range(len(bin_sums)):\n",
    "            plot_data.append({\n",
    "                \"Dataset\": dataset,\n",
    "                \"Velocity Bin\": bin_labels[i],\n",
    "                \"Sum of Deviations\": bin_sums[i]\n",
    "            })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Pivot the DataFrame for easier plotting\n",
    "    pivot_df = plot_df.pivot(index=\"Velocity Bin\", columns=\"Dataset\", values=\"Sum of Deviations\").fillna(0)\n",
    "\n",
    "    # Plotting\n",
    "    pivot_df.plot(kind='bar', figsize=(15, 8))\n",
    "    plt.xlabel(\"Ground Truth Velocity Bin (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Sum of Velocity Deviations (m/s)\", fontsize=14)\n",
    "    plt.title(\"Sum of Velocity Deviations per Ground Truth Velocity Bin with Treshold=0.01\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_deviation_frequency(deviation_frequency, output_path, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Plot the frequency of deviations per ground truth velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                                    mapping ground truth velocity to deviation count.\n",
    "        output_path (str): Path to save the deviation frequency plot.\n",
    "        bin_size (float): Size of each velocity bin.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define fixed velocity bins from 0 to 0.56 m/s with bin size 0.05 m/s\n",
    "    min_v = 0.0\n",
    "    max_v = 0.56\n",
    "    bins = np.arange(min_v, max_v + bin_size, bin_size)\n",
    "    bin_labels = [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
    "\n",
    "    # Prepare DataFrame for plotting\n",
    "    plot_data = []\n",
    "    for dataset, freq_dict in deviation_frequency.items():\n",
    "        # Initialize counts for each bin\n",
    "        bin_counts = np.zeros(len(bins)-1, dtype=int)\n",
    "        for v, count in freq_dict.items():\n",
    "            # Find the appropriate bin\n",
    "            bin_index = np.digitize(v, bins) - 1  # digitize returns indices starting at 1\n",
    "            if 0 <= bin_index < len(bin_counts):\n",
    "                bin_counts[bin_index] += count\n",
    "        for i in range(len(bin_counts)):\n",
    "            plot_data.append({\n",
    "                \"Dataset\": dataset,\n",
    "                \"Velocity Bin\": bin_labels[i],\n",
    "                \"Frequency\": bin_counts[i]\n",
    "            })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Pivot the DataFrame for easier plotting\n",
    "    pivot_df = plot_df.pivot(index=\"Velocity Bin\", columns=\"Dataset\", values=\"Frequency\").fillna(0)\n",
    "\n",
    "    # Plotting\n",
    "    pivot_df.plot(kind='bar', figsize=(15, 8))\n",
    "    plt.xlabel(\"Ground Truth Velocity Bin (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Frequency of Deviations\", fontsize=14)\n",
    "    plt.title(\"Frequency of Velocity Deviations per Ground Truth Velocity Bin with Treshold=0.01\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_fixed_velocity_bin_metrics(deviation_frequency, deviation_sum, output_path, bin_label=\"0.00-0.10 m/s\"):\n",
    "    \"\"\"\n",
    "    Plot frequency, sum, and density of deviations within a specific velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                                    mapping ground truth velocity to deviation count.\n",
    "        deviation_sum (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                              mapping ground truth velocity to sum of deviation magnitudes.\n",
    "        output_path (str): Path to save the fixed velocity bin metrics plot.\n",
    "        bin_label (str): The specific velocity bin to focus on (e.g., \"0.00-0.10 m/s\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define the specific velocity bin\n",
    "    target_bin = bin_label  # e.g., \"0.00-0.10 m/s\"\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for dataset in deviation_frequency:\n",
    "        # Extract the lower and upper bounds from bin_label\n",
    "        try:\n",
    "            lower_str, upper_str = target_bin.split('-')\n",
    "            # Remove any non-numeric characters from upper_str\n",
    "            upper_str = ''.join(filter(lambda x: x.isdigit() or x == '.', upper_str))\n",
    "            lower = float(lower_str)\n",
    "            upper = float(upper_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing bin_label '{bin_label}': {e}\")\n",
    "            continue\n",
    "\n",
    "        # Count frequency\n",
    "        freq = 0\n",
    "        for v, count in deviation_frequency[dataset].items():\n",
    "            if lower <= v < upper:\n",
    "                freq += count\n",
    "\n",
    "        # Sum of deviations\n",
    "        sum_dev = 0.0\n",
    "        for v, s_dev in deviation_sum[dataset].items():\n",
    "            if lower <= v < upper:\n",
    "                sum_dev += s_dev\n",
    "\n",
    "        # Total deviations for density\n",
    "        total_devs = sum(deviation_frequency[dataset].values())\n",
    "        density = (freq / total_devs) * 100 if total_devs > 0 else 0\n",
    "\n",
    "        plot_data.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Frequency\": freq,\n",
    "            \"Sum of Deviations\": sum_dev,\n",
    "            \"Density (%)\": density\n",
    "        })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Check if plot_df is empty\n",
    "    if plot_df.empty:\n",
    "        print(f\"No data available for the bin '{bin_label}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # Set the order for metrics\n",
    "    metrics_order = [\"Frequency\", \"Sum of Deviations\", \"Density (%)\"]\n",
    "\n",
    "    # Melt the DataFrame for seaborn plotting\n",
    "    plot_df_melted = plot_df.melt(id_vars=\"Dataset\", value_vars=metrics_order, var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=plot_df_melted, x=\"Dataset\", y=\"Value\", hue=\"Metric\")\n",
    "    plt.xlabel(\"Dataset\", fontsize=14)\n",
    "    plt.ylabel(\"Value\", fontsize=14)\n",
    "    plt.title(f\"Deviation Metrics within {target_bin} Velocity Bin\", fontsize=16)\n",
    "    plt.legend(title=\"Metric\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "import re  # Add this import at the top of your script\n",
    "\n",
    "def plot_fixed_velocity_bin_metrics(deviation_frequency, deviation_sum, output_path, bin_label=\"0.00-0.10 m/s\"):\n",
    "    \"\"\"\n",
    "    Plot frequency, sum, and density of deviations within a specific velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                                    mapping ground truth velocity to deviation count.\n",
    "        deviation_sum (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                              mapping ground truth velocity to sum of deviation magnitudes.\n",
    "        output_path (str): Path to save the fixed velocity bin metrics plot.\n",
    "        bin_label (str): The specific velocity bin to focus on (e.g., \"0.00-0.10 m/s\").\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define the specific velocity bin\n",
    "    target_bin = bin_label  # e.g., \"0.00-0.10 m/s\"\n",
    "\n",
    "    # Use regular expressions to extract numeric boundaries\n",
    "    match = re.match(r\"([0-9.]+)-([0-9.]+)\", target_bin)\n",
    "    if not match:\n",
    "        print(f\"Invalid bin_label format: '{bin_label}'. Expected format 'lower-upper m/s'.\")\n",
    "        return\n",
    "\n",
    "    lower_str, upper_str = match.groups()\n",
    "    try:\n",
    "        lower = float(lower_str)\n",
    "        upper = float(upper_str)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting bin boundaries to float: {e}\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for dataset in deviation_frequency:\n",
    "        # Count frequency\n",
    "        freq = 0\n",
    "        for v, count in deviation_frequency[dataset].items():\n",
    "            if lower <= v < upper:\n",
    "                freq += count\n",
    "\n",
    "        # Sum of deviations\n",
    "        sum_dev = 0.0\n",
    "        for v, s_dev in deviation_sum[dataset].items():\n",
    "            if lower <= v < upper:\n",
    "                sum_dev += s_dev\n",
    "\n",
    "        # Total deviations for density\n",
    "        total_devs = sum(deviation_frequency[dataset].values())\n",
    "        density = (freq / total_devs) * 100 if total_devs > 0 else 0\n",
    "\n",
    "        plot_data.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Frequency\": freq,\n",
    "            \"Sum of Deviations\": sum_dev,\n",
    "            \"Density (%)\": density\n",
    "        })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Check if plot_df is empty\n",
    "    if plot_df.empty:\n",
    "        print(f\"No data available for the bin '{bin_label}'. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    # Set the order for metrics\n",
    "    metrics_order = [\"Frequency\", \"Sum of Deviations\", \"Density (%)\"]\n",
    "\n",
    "    # Melt the DataFrame for seaborn plotting\n",
    "    plot_df_melted = plot_df.melt(id_vars=\"Dataset\", value_vars=metrics_order, var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=plot_df_melted, x=\"Dataset\", y=\"Value\", hue=\"Metric\")\n",
    "    plt.xlabel(\"Dataset\", fontsize=14)\n",
    "    plt.ylabel(\"Value\", fontsize=14)\n",
    "    plt.title(f\"Deviation Metrics within {target_bin} Velocity Bin\", fontsize=16)\n",
    "    plt.legend(title=\"Metric\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_ground_truth_velocity_density_continuous_first_dataset(ground_truth_density, output_path):\n",
    "    \"\"\"\n",
    "    Plot the continuous density distribution of ground truth velocities for the first dataset.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_density (dict): Dictionary with dataset names as keys and lists of ground truth velocities as values.\n",
    "        output_path (str): Path to save the continuous ground truth velocity density plot.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Check if ground_truth_density is not empty\n",
    "    if not ground_truth_density:\n",
    "        print(\"The ground_truth_density dictionary is empty. No data to plot.\")\n",
    "        return\n",
    "    \n",
    "    # Get the first dataset\n",
    "    first_dataset = next(iter(ground_truth_density))\n",
    "    velocities = ground_truth_density[first_dataset]\n",
    "    \n",
    "    if len(velocities) == 0:\n",
    "        print(f\"No ground truth velocities available for dataset '{first_dataset}'. Skipping plot.\")\n",
    "        return\n",
    "    \n",
    "    # Define color palette\n",
    "    color = sns.color_palette(\"tab10\", n_colors=1)[0]\n",
    "    \n",
    "    # Plot KDE for the first dataset\n",
    "    sns.kdeplot(\n",
    "        velocities, \n",
    "        label=first_dataset, \n",
    "        shade=True, \n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        bw_adjust=1  # Adjust bandwidth for smoothness\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(\"Ground Truth Velocity (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Density\", fontsize=14)\n",
    "    plt.title(f\"Continuous Density Distribution of Ground Truth Velocities\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlim(0, 0.56)  # Set x-axis limits from 0 to 0.56 m/s\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Continuous ground truth velocity density plot for '{first_dataset}' saved to '{output_path}'.\")\n",
    "\n",
    "\n",
    "def plot_deviation_density_per_velocity_bin(deviation_frequency, output_path, bin_size=0.05):\n",
    "    \"\"\"\n",
    "    Plot the density of deviations per ground truth velocity bin for each dataset.\n",
    "\n",
    "    Args:\n",
    "        deviation_frequency (dict): Dictionary with dataset names as keys and another dict as values\n",
    "                                    mapping ground truth velocity to deviation count.\n",
    "        output_path (str): Path to save the deviation density plot.\n",
    "        bin_size (float): Size of each velocity bin.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define fixed velocity bins from 0 to 0.56 m/s with bin size 0.05 m/s\n",
    "    min_v = 0.0\n",
    "    max_v = 0.56\n",
    "    bins = np.arange(min_v, max_v + bin_size, bin_size)\n",
    "    bin_labels = [f\"{bins[i]:.2f}-{bins[i+1]:.2f}\" for i in range(len(bins)-1)]\n",
    "\n",
    "    # Prepare DataFrame for plotting\n",
    "    plot_data = []\n",
    "    for dataset, freq_dict in deviation_frequency.items():\n",
    "        # Initialize counts for each bin\n",
    "        bin_counts = np.zeros(len(bins)-1, dtype=int)\n",
    "        for v, count in freq_dict.items():\n",
    "            # Find the appropriate bin\n",
    "            bin_index = np.digitize(v, bins) - 1  # digitize returns indices starting at 1\n",
    "            if 0 <= bin_index < len(bin_counts):\n",
    "                bin_counts[bin_index] += count\n",
    "        # Calculate total deviations for density normalization\n",
    "        total_deviations = bin_counts.sum()\n",
    "        if total_deviations > 0:\n",
    "            bin_density = (bin_counts / total_deviations) * 100  # Percentage\n",
    "        else:\n",
    "            bin_density = bin_counts  # All zeros\n",
    "        for i in range(len(bin_density)):\n",
    "            plot_data.append({\n",
    "                \"Dataset\": dataset,\n",
    "                \"Velocity Bin\": bin_labels[i],\n",
    "                \"Density (%)\": bin_density[i]\n",
    "            })\n",
    "\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    # Pivot the DataFrame for easier plotting\n",
    "    pivot_df = plot_df.pivot(index=\"Velocity Bin\", columns=\"Dataset\", values=\"Density (%)\").fillna(0)\n",
    "\n",
    "    # Plotting\n",
    "    pivot_df.plot(kind='bar', figsize=(15, 8))\n",
    "    plt.xlabel(\"Ground Truth Velocity Bin (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Density of Deviations (%)\", fontsize=14)\n",
    "    plt.title(\"Density of Velocity Deviations per Ground Truth Velocity Bin\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def generate_aggregate_plots(aggregate_results, aggregate_plots_folder, bin_size=0.05, velocity_threshold=0.35):\n",
    "    \"\"\"\n",
    "    Generate aggregate plots comparing datasets, including average deviation per bin considering a velocity threshold.\n",
    "\n",
    "    Args:\n",
    "        aggregate_results (dict): Contains accumulated data for aggregate plots.\n",
    "        aggregate_plots_folder (str): Directory to save the aggregate plots.\n",
    "        bin_size (float): Size of each velocity bin for the frequency and sum plots.\n",
    "        velocity_threshold (float): The minimum ground truth velocity to consider (in m/s).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(aggregate_plots_folder, exist_ok=True)\n",
    "\n",
    "    # Unpack aggregate data\n",
    "    aggregate_deviations = aggregate_results[\"aggregate_deviations\"]\n",
    "    aggregate_high_velocity_deviations = aggregate_results[\"aggregate_high_velocity_deviations\"]\n",
    "    aggregate_deviation_histograms = aggregate_results[\"aggregate_deviation_histograms\"]\n",
    "    deviation_frequency = aggregate_results[\"deviation_frequency\"]\n",
    "    deviation_sum = aggregate_results[\"deviation_sum\"]\n",
    "    ground_truth_density = aggregate_results[\"ground_truth_density\"]\n",
    "\n",
    "    # Generate Aggregate Overall Deviation Plot\n",
    "    aggregate_overall_deviation_path = os.path.join(aggregate_plots_folder, \"aggregate_overall_deviation.png\")\n",
    "    plot_aggregate_overall_deviation(aggregate_deviations, aggregate_overall_deviation_path, bins=50)\n",
    "\n",
    "    # Generate Aggregate High-Velocity Deviation Plot\n",
    "    aggregate_high_velocity_deviation_path = os.path.join(aggregate_plots_folder, \"aggregate_high_velocity_deviation.png\")\n",
    "    plot_aggregate_high_velocity_deviation(aggregate_high_velocity_deviations, aggregate_high_velocity_deviation_path, bins=50)\n",
    "\n",
    "    # Generate Aggregate Error by Fixed Velocity Regions (Frequency-based)\n",
    "    aggregate_error_by_regions_frequency_path = os.path.join(aggregate_plots_folder, \"aggregate_error_by_fixed_velocity_regions_frequency.png\")\n",
    "    plot_deviation_frequency(deviation_frequency, aggregate_error_by_regions_frequency_path, bin_size=bin_size)\n",
    "\n",
    "    # Generate Aggregate Error by Fixed Velocity Regions (Sum-based)\n",
    "    aggregate_error_by_regions_sum_path = os.path.join(aggregate_plots_folder, \"aggregate_error_by_fixed_velocity_regions_sum.png\")\n",
    "    plot_sum_deviation_per_velocity_bin(deviation_sum, aggregate_error_by_regions_sum_path, bin_size=bin_size)\n",
    "\n",
    "    # Generate Aggregate Error by Fixed Velocity Regions (Density-based)\n",
    "    aggregate_error_by_regions_density_path = os.path.join(aggregate_plots_folder, \"aggregate_error_by_fixed_velocity_regions_density.png\")\n",
    "    plot_deviation_density_per_velocity_bin(deviation_frequency, aggregate_error_by_regions_density_path, bin_size=bin_size)\n",
    "\n",
    "    # Generate Fixed Velocity Bin Metrics Plot for 0.00-0.10 m/s\n",
    "    fixed_velocity_bin = \"0.00-0.10 m/s\"\n",
    "    aggregate_error_by_regions_0_0_1_path = os.path.join(aggregate_plots_folder, \"aggregate_error_by_fixed_velocity_regions_0_0_1.png\")\n",
    "    plot_fixed_velocity_bin_metrics(deviation_frequency, deviation_sum, aggregate_error_by_regions_0_0_1_path, bin_label=fixed_velocity_bin)\n",
    "\n",
    "    # Generate Ground Truth Velocity Density Plot (Continuous for First Dataset)\n",
    "    ground_truth_velocity_density_continuous_first_dataset_path = os.path.join(\n",
    "        aggregate_plots_folder, \"ground_truth_velocity_density_continuous_first_dataset.png\"\n",
    "    )\n",
    "    plot_ground_truth_velocity_density_continuous_first_dataset(\n",
    "        aggregate_results[\"ground_truth_density\"], \n",
    "        ground_truth_velocity_density_continuous_first_dataset_path\n",
    "    )\n",
    "\n",
    "    # **New Steps:** Compute Average Deviations with Velocity Threshold and Generate the Plot\n",
    "    # Compute the average deviation per bin per dataset considering the velocity threshold\n",
    "    avg_dev_df = compute_average_deviation(deviation_frequency, deviation_sum, velocity_threshold=velocity_threshold)\n",
    "    \n",
    "    # Define the output path for the average deviation plot\n",
    "    average_deviation_plot_path = os.path.join(aggregate_plots_folder, f\"average_deviation_per_bin_per_dataset_above_{velocity_threshold}_m_s.png\")\n",
    "    \n",
    "    # Generate the plot with the velocity threshold\n",
    "    plot_average_deviation_per_bin(avg_dev_df, average_deviation_plot_path, velocity_threshold=velocity_threshold)\n",
    "\n",
    "    # Generate Aggregate Deviation Histogram\n",
    "    aggregate_deviation_histogram_path = os.path.join(aggregate_plots_folder, \"aggregate_deviation_histogram.png\")\n",
    "    plot_aggregate_deviation_histogram(aggregate_deviation_histograms, aggregate_deviation_histogram_path, bins=50)\n",
    "\n",
    "    print(\"\\nAggregate plots generated and saved successfully.\")\n",
    "\n",
    "\n",
    "def plot_aggregate_overall_deviation(aggregate_deviations, output_path, bins=50):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.tab10.colors  # Up to 10 distinct colors\n",
    "\n",
    "    for idx, (dataset, deviations) in enumerate(aggregate_deviations.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.hist(deviations, bins=bins, alpha=0.5, color=color, label=dataset, density=True)\n",
    "\n",
    "    plt.xlabel(\"Velocity Deviation (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Density\", fontsize=14)\n",
    "    plt.title(\"Aggregate Velocity Deviations Across All Datasets\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_aggregate_high_velocity_deviation(aggregate_high_velocity_deviations, output_path, bins=50):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.tab10.colors  # Up to 10 distinct colors\n",
    "\n",
    "    for idx, (dataset, deviations) in enumerate(aggregate_high_velocity_deviations.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.hist(deviations, bins=bins, alpha=0.5, color=color, label=dataset, density=True)\n",
    "\n",
    "    plt.xlabel(\"High-Velocity Deviation (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Density\", fontsize=14)\n",
    "    plt.title(\"Aggregate High-Velocity Deviations Across All Datasets\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_aggregate_deviation_histogram(aggregate_deviation_histograms, output_path, bins=50):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.tab10.colors  # Up to 10 distinct colors\n",
    "\n",
    "    for idx, (dataset, deviations) in enumerate(aggregate_deviation_histograms.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        plt.hist(deviations, bins=bins, alpha=0.5, color=color, label=dataset, density=True)\n",
    "\n",
    "    plt.xlabel(\"Velocity Deviation (m/s)\", fontsize=14)\n",
    "    plt.ylabel(\"Density\", fontsize=14)\n",
    "    plt.title(\"Aggregate Deviation Histograms Across All Datasets\", fontsize=16)\n",
    "    plt.legend(title=\"Dataset\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing to analyze Dataset: 'DAT_x2_Scalar_No_Physics'\n",
      "\n",
      "Preparing to analyze Dataset: 'DAT_X2_Vector_No_Physics'\n",
      "\n",
      "Preparing to analyze Dataset: 'DAT_X2_Vector_umass_2'\n",
      "\n",
      "Starting parallel processing with all available jobs...\n",
      "Ground truth for 'XZ_127_35.png' not found in dataset 'DAT_x2_Scalar_No_Physics'. Skipping.\n",
      "Ground truth for 'XZ_30_35.png' not found in dataset 'DAT_x2_Scalar_No_Physics'. Skipping.\n",
      "Ground truth for 'XZ_127_38.png' not found in dataset 'DAT_x2_Scalar_No_Physics'. Skipping.\n",
      "Ground truth for 'XZ_30_34.png' not found in dataset 'DAT_x2_Scalar_No_Physics'. Skipping.\n",
      "Ground truth for 'XZ_127_34.png' not found in dataset 'DAT_x2_Scalar_No_Physics'. Skipping.\n",
      "\n",
      "--- Metrics DataFrame Head ---\n",
      "                    Dataset         Image       MAE       MSE      RMSE  \\\n",
      "0  DAT_x2_Scalar_No_Physics  XZ_63_35.png  0.004820  0.000117  0.010814   \n",
      "1  DAT_x2_Scalar_No_Physics  XZ_43_35.png  0.003922  0.000115  0.010740   \n",
      "2  DAT_x2_Scalar_No_Physics  XZ_83_39.png  0.002206  0.000023  0.004836   \n",
      "3  DAT_x2_Scalar_No_Physics  XZ_53_31.png  0.002124  0.000018  0.004236   \n",
      "4  DAT_x2_Scalar_No_Physics  XZ_66_40.png  0.006046  0.000170  0.013031   \n",
      "\n",
      "         R2  \n",
      "0  0.992019  \n",
      "1  0.929875  \n",
      "2  0.984934  \n",
      "3  0.996407  \n",
      "4  0.957764  \n",
      "\n",
      "--- Metrics DataFrame Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 755 entries, 0 to 754\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Dataset  755 non-null    object \n",
      " 1   Image    755 non-null    object \n",
      " 2   MAE      755 non-null    float64\n",
      " 3   MSE      755 non-null    float64\n",
      " 4   RMSE     755 non-null    float64\n",
      " 5   R2       755 non-null    float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 35.5+ KB\n",
      "None\n",
      "\n",
      "--- Unique Dataset Names ---\n",
      "['DAT_x2_Scalar_No_Physics' 'DAT_X2_Vector_No_Physics'\n",
      " 'DAT_X2_Vector_umass_2']\n",
      "\n",
      "--- Entries in 'Dataset' Containing Image Filenames ---\n",
      "Empty DataFrame\n",
      "Columns: [Dataset, Image, MAE, MSE, RMSE, R2]\n",
      "Index: []\n",
      "\n",
      "--- Length of 'Dataset' Entries ---\n",
      "count    755.000000\n",
      "mean      22.998675\n",
      "std        1.415619\n",
      "min       21.000000\n",
      "25%       21.000000\n",
      "50%       24.000000\n",
      "75%       24.000000\n",
      "max       24.000000\n",
      "Name: Dataset, dtype: float64\n",
      "\n",
      "No unusually long 'Dataset' entries found.\n",
      "\n",
      "No problematic 'Dataset' entries found. Proceeding...\n",
      "\n",
      "--- Average Metrics DataFrame ---\n",
      "                    Dataset       MAE       MSE      RMSE        R2\n",
      "0  DAT_X2_Vector_No_Physics  0.005726  0.000192  0.010915  0.957423\n",
      "1     DAT_X2_Vector_umass_2  0.005662  0.000183  0.010731  0.957557\n",
      "2  DAT_x2_Scalar_No_Physics  0.005116  0.000192  0.011965  0.948251\n",
      "\n",
      "Average error metrics per dataset saved to '/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/output_folder/average_error_metrics.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8103/4252552799.py:864: FutureWarning: \n",
      "\n",
      "`shade` is now deprecated in favor of `fill`; setting `fill=True`.\n",
      "This will become an error in seaborn v0.14.0; please update your code.\n",
      "\n",
      "  sns.kdeplot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous ground truth velocity density plot for 'DAT_x2_Scalar_No_Physics' saved to '/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/output_folder/aggregate_plots/ground_truth_velocity_density_continuous_first_dataset.png'.\n",
      "Average deviation per bin plot saved to '/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/output_folder/aggregate_plots/average_deviation_per_bin_per_dataset_above_0.35_m_s.png'.\n",
      "\n",
      "Aggregate plots generated and saved successfully.\n",
      "\n",
      "All analyses and aggregate plots completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define model prediction folders\n",
    "model_paths = {\n",
    "    \"DAT_x2_Scalar_No_Physics\": \"/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/DAT_x2_Scalar_No_Physics\",\n",
    "    \"DAT_X2_Vector_No_Physics\": \"/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/DAT_X2_Vector_No_Physics\",\n",
    "    \"DAT_X2_Vector_umass_2\": \"/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/DAT_X2_Vector_umass_2\"\n",
    "}\n",
    "\n",
    "# Define ground truth images folder\n",
    "ground_truth_folder = \"/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/GroundTruth\"\n",
    "\n",
    "# Define output folder for analysis results\n",
    "output_folder = \"/home/vittorio/Scrivania/ResShift_4_scale/data/AnalysisVelocity/output_folder\"\n",
    "\n",
    "deviation_threshold = 0.01 # Adjust based on your requirements\n",
    "\n",
    "\n",
    "# Perform batch comprehensive analysis with parallel processing\n",
    "analysis_results = batch_comprehensive_analysis_parallel(\n",
    "    dataset_paths=model_paths,\n",
    "    ground_truth_folder=ground_truth_folder,\n",
    "    velocity_threshold=0.4, \n",
    "    deviation_threshold=deviation_threshold,\n",
    " # Adjust as needed\n",
    "    n_jobs=-1  # Use all available processors\n",
    ")\n",
    "\n",
    "# Extract average metrics DataFrame\n",
    "average_metrics_df = analysis_results[\"average_metrics_df\"]\n",
    "\n",
    "# Debugging: Inspect the average_metrics_df\n",
    "print(\"\\n--- Average Metrics DataFrame ---\")\n",
    "print(average_metrics_df)\n",
    "\n",
    "# Save average error metrics to CSV\n",
    "metrics_csv_path = os.path.join(output_folder, \"average_error_metrics.csv\")\n",
    "average_metrics_df.to_csv(metrics_csv_path, index=False)\n",
    "print(f\"\\nAverage error metrics per dataset saved to '{metrics_csv_path}'.\")\n",
    "\n",
    "# Define paths for aggregate plots\n",
    "aggregate_plots_folder = os.path.join(output_folder, \"aggregate_plots\")\n",
    "os.makedirs(aggregate_plots_folder, exist_ok=True)\n",
    "\n",
    "# Generate Aggregate Plots\n",
    "generate_aggregate_plots(analysis_results, aggregate_plots_folder, bin_size=0.05)\n",
    "\n",
    "print(\"\\nAll analyses and aggregate plots completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resShift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
